#hide
# !pip install -Uqq fastbook
import fastbook
fastbook.setup_book()

#hide
from fastbook import *
from fastai.vision.widgets import *

droplet_types = 'Centered Cell Aggregates','Non-Centered Cell Aggregates', 'Centered Spheroids', 'Non-Centered Spheroids', 'Single Cells', 'Objects Out Of Focus'
path = Path('Droplets')

## From Data to DataLoaders

droplets = DataBlock(
    blocks=(ImageBlock, CategoryBlock), 
    get_items=get_image_files, 
    splitter=RandomSplitter(valid_pct=0.2, seed=32),
    get_y=parent_label,
    item_tfms=Resize(128))

dls = droplets.dataloaders(path)
learn = cnn_learner(dls, resnet50, metrics=error_rate)
lr_min,lr_steep = learn.lr_find()

def find_appropriate_lr(model:Learner, lr_diff:int = 15, loss_threshold:float = .05, adjust_value:float = 1, plot:bool = False) -> float:
    #Run the Learning Rate Finder
    model.lr_find()
    
    #Get loss values and their corresponding gradients, and get lr values
    losses = np.array(model.recorder.losses)
    assert(lr_diff < len(losses))
    loss_grad = np.gradient(losses)
    lrs = model.recorder.lrs
    
    #Search for index in gradients where loss is lowest before the loss spike
    #Initialize right and left idx using the lr_diff as a spacing unit
    #Set the local min lr as -1 to signify if threshold is too low
    r_idx = -1
    l_idx = r_idx - lr_diff
    while (l_idx >= -len(losses)) and (abs(loss_grad[r_idx] - loss_grad[l_idx]) > loss_threshold):
        local_min_lr = lrs[l_idx]
        r_idx -= 1
        l_idx -= 1

    lr_to_use = local_min_lr * adjust_value
    
    if plot:
        # plots the gradients of the losses in respect to the learning rate change
        plt.plot(loss_grad)
        plt.plot(len(losses)+l_idx, loss_grad[l_idx],markersize=10,marker='o',color='red')
        plt.ylabel("Loss")
        plt.xlabel("Index of LRs")
        plt.show()

        plt.plot(np.log10(lrs), losses)
        plt.ylabel("Loss")
        plt.xlabel("Log 10 Transform of Learning Rate")
        loss_coord = np.interp(np.log10(lr_to_use), np.log10(lrs), losses)
        plt.plot(np.log10(lr_to_use), loss_coord, markersize=10,marker='o',color='red')
        plt.show()
        
    return lr_to_use

find_appropriate_lr(learn)

## Training Your Model, and Using It to Clean Your Data

droplets = droplets.new(
    item_tfms=RandomResizedCrop(128, min_scale=0.8),
    batch_tfms=aug_transforms(
    mult=2.0,
    do_flip=True,
    flip_vert=True,
    max_rotate=180.0,
    min_zoom=1.0,
    max_zoom=1.5,
    max_lighting=0.2,
    max_warp=0.2,
    p_affine=0.75,
    p_lighting=0.75,
    xtra_tfms=None,
    size=None,
    mode='bilinear',
    pad_mode='reflection',
    align_corners=True,
    batch=False,
    min_scale=1.0,
))
dls = droplets.dataloaders(path)

learn = cnn_learner(dls, resnet50, metrics=error_rate,path=Path.cwd()/'tmp')
learn.fine_tune(50, base_lr=0.009120108393559097, cbs=SaveModelCallback(monitor='error_rate'))
assert (Path.cwd()/'tmp/models/model.pth').exists()

learn.export()

path = Path()
path.ls(file_exts='.pkl')
